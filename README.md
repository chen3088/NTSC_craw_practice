# NTSC_craw_practice

This repository contains a simple practice project for web crawling job postings from the National Science and Technology Council (NSTC) of Taiwan.

## Contents

- `NTSC_job_data.ipynb` – Jupyter notebook that fetches job posting pages and parses them with `requests` and `BeautifulSoup`.
- `nstc_jobs_partial.csv` – A partial dataset of jobs already collected as a reference.
- `RAG_mini.ipynb` – Demonstrates a lightweight retrieval-augmented generation workflow using FAISS.
- `RAG_sercher.ipynb` – A more complete RAG example that builds a FAISS index and logs query results.
- `nstc_jobs_full.csv` – The full dataset of job postings scraped from the NSTC website.
- Prebuilt FAISS indexes (`faiss_index.index`, `faiss_recursive.index`, `faiss_hybrid.index`) and their accompanying doc files are provided for convenience.
- Query logs such as `rag_results_log.csv` and `rag_results_log.xlsx` are generated by the searcher notebook.

## Requirements

The notebook relies on the following Python packages:

- `requests`
- `pandas`
- `beautifulsoup4`

Install them with `pip install requests pandas beautifulsoup4` if they are not already available.

### RAG_mini.ipynb Requirements

The `RAG_mini.ipynb` notebook demonstrates a minimal retrieval-augmented generation example. In addition to `pandas`, it relies on a few extra libraries:

- `langchain`
- `sentence-transformers`
- `faiss-cpu`
- `numpy`

Install them with `pip install langchain sentence-transformers faiss-cpu numpy`.

### RAG_sercher.ipynb Parameters

The `RAG_sercher.ipynb` notebook expands on the minimal example by building a FAISS index and logging searches.
Important variables are defined near the top of the notebook:

- `file_path` – CSV file used to load the job data.
- `model_name` – Sentence-transformers model for embedding generation.
- `chunk_size` & `chunk_overlap` – Control how documents are split before indexing.
- `encoding_batch_size` – Batch size when encoding text.
- `top_k` – Number of documents returned for each query.
- `output_log_path` – Where to save the CSV log of search results.

## Usage

1. Clone this repository and open `NTSC_job_data.ipynb` with Jupyter Notebook or JupyterLab.
2. Run through the cells to crawl the latest job postings from the NSTC website.
3. The notebook saves the full dataset to `nstc_jobs_full.csv` once the crawl completes.
4. Open `RAG_sercher.ipynb` to build (or load) the FAISS index and run search queries. Results are saved to `rag_results_log.csv` and `rag_results_log.xlsx`.

The provided `nstc_jobs_partial.csv` file is a small snapshot of the data which can be used to test the parsing logic before performing a full crawl.

## License

This project is intended for learning purposes only. Please respect the terms of the NSTC website when running the crawler.
