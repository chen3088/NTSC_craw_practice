舉例來說，每逢有人因車禍不幸橫死，當記者報導死者是孝子，我們常唏噓說為何橫死的都是好人？這樣的想法，其實犯了諾貝爾經濟學獎得主、心理學家 Daniel Kahneman 所說的「基率謬誤」（base rate fallacy）。簡單來說，就是沒有把「絕大多數人都是好人」這個「基率」——**貝氏定理所謂的先驗機率（prior probability）**——納入考量所致。因為絕大多數人都是好人，即使老天爺真的大致上賞善罰惡，橫死的人也會大多是好人，更不用說車禍應該跟善惡無關了。

比如我們假設每100人中只有1人（1%）是十惡不赦的「壞人」，其餘99人（99%）都是「好人」。再假設90%的壞人果然都遭車禍橫死，而只有10%的好人意外橫死。這樣老天算是有眼了，可是如果今天有人意外橫死，請問他是好人的機率多少呢？用貝氏定理可以算出Pr(好人|橫死)=0.92，也就是橫死的人中有92%會是「好人」，只有8%是壞人！這正是因為大部分人都是好人，出事的當然容易是好人，即使老天有眼也是一樣。

貝氏定理的原理就是在先驗機率的基礎上，納入新事件的資訊來更新先驗機率，這樣算出來的機率便叫做後驗機率（posterior probability）。以前述好人橫死的例子來說，先驗機率的分配是 Pr(好人)=0.99及Pr(壞人)=0.01。在無其他資訊的情況下，我們在街上隨機遇到一個人，此人為好人的機率是0.99。

- ----廣告，請繼續往下閱讀-----

但現在此人被車子撞死了，根據我們對老天有眼的假設（Pr(橫死|好人)=0.1 及 Pr(橫死|壞人)=0.9），好人不容易橫死，而此人橫死了，這新事件的資訊可以讓我們用貝氏定理來計算後驗機率 Pr(好人|橫死)=0.92，也就是此人為好人的機率變小。新事件的資訊改變了我們原來的估計，這就是所謂「貝氏更新」（Bayesian updating）。

圖／[makeagif](https://makeagif.com/i/t2CrGI)

如果我們沒有把先驗機率納入計算，我們很可能因為相信老天有眼，橫死的應該大多是壞人，就斷此人很可能是壞人。而若確定此人是好人，我們就唏噓不已，甚至怨罵老天。這兩種反應的人其實都犯了「基率謬誤」。當然，如果車禍跟人的好壞無關，也就是不論好人壞人橫死的機率都一樣，則有人橫死的新事件是不會更新我們對他是好人或壞人的基率的。

Kahneman在《快思慢想》一書中舉了一個也是跟車禍有關的「基率謬誤」的例子。某天夜晚城裡發生了一件車禍，肇事的車子逃逸，但有證人指認那是一輛藍色的計程車。城裡只有藍色、綠色兩種計程車；綠色車佔85%，藍色車僅佔15%。法庭檢驗證人在夜晚識別車色的能力，發現他識別正確的機率是80%，而識別錯誤的機率是20%。

當Kahneman做實驗問受測者肇事車輛為藍色的機率多少時，大部分人的答案是80%。這也是犯了「基率謬誤」的答案，也就是城裡「綠色車佔85%，藍色車佔15%」這個基率所包含的資訊被忽略了。如果把基率納入考量，貝氏定理給的答案是Pr(肇事車真為藍色|證人指認為藍色)=0.41，只有一般人想像中的一半！

- ----廣告，請繼續往下閱讀-----

現實生活中類似的例子很多：身體檢查某項檢驗得到陽性反應、職棒大聯盟球員沒通過藥檢、犯罪現場採得的DNA與調查局資料庫CODIS中某人的DNA相符、甚至統計上Ｐ值檢定得到顯著結果。這些情況中，如果我們不了解貝氏定理，我們很可能就會在機率估計上犯錯。那麼貝氏定理究竟要如何拿來計算正確的後驗機率呢？本文將用淺易的途徑來介紹貝氏定理的計算方法。

## 聯合機率、邊際機率以及條件機率三種必須認識的機率

欲瞭解貝式定理的邏輯，必須先瞭解三種不同的機率：聯合機率（joint probability）、邊際機率（marginal probability）以及條件機率（conditional probability）。

假設有兩個隨機變數（random variable）X和Y，變數X有1, 2, …, J共J個可能的值，而變數Y有1, 2, …, I共I個值。在此可以將變數的「值」視為前面提及的「事件」（event），舉例來說，X代表大聯盟球員有沒有使用禁藥，X=1代表「沒有使用」，X=2代表「有使用」；Y代表藥檢的結果，Y=1代表「陽性反應」，Y=2代表「陰性反應」。這裡X=1、X=2、Y=1、Y=2都是其發生有一定機率的事件。

如果我們想要檢視X和Y之間的關係，可以繪製出下列交叉表：

- ----廣告，請繼續往下閱讀-----

我們先從概念開始介紹。表一所陳列的Y跟X聯合起來所有可能的結果可以用 {(1,1), (1,2), …, (i,j), …, (I,J)} 這個集合來表示，這就是Y跟X聯合起來的「樣本空間」，它一共有IxJ個可能結果。每一個結果所對應的機率是Y跟X的**聯合機率**，也就是屬於Y的事件Y=i和屬於X的事件X=j聯合發生的機率，數學表示為Pr(Y=i,X=j)=πij。例如π11就是Y=1和X=1這兩個事件都發生的機率，π12則是Y=1和X=2這兩個事件都發生的機率，以此類推。如果我們把所有可能結果的機率加總，從π11加到πIJ，總和必須是1。

**邊際機率**則是屬於Y或X的單一事件發生的機率。表一中，Y的樣本空間是 {1, 2, …, i, …, I}；屬於Y的事件發生的邊際機率用Pr(Y=i)=πi.表示。X的樣本空間是 {1, 2, …, j, …, J}；屬於X的事件發生的邊際機率用Pr(X=j)=π.j表示。例如π１．就是Y=1這個事件發生的機率，π.2則是X=2這個事件發生的機率，以此類推。Y或X所有邊際機率的總和也必須是1。在表一裡，我們以行或列的總和來計算邊際機率。邊際機率其實就是單一變數的機率分配，之所以稱為邊際機率指是因為我們從表一的雙變數聯合機率分配的脈絡出發，導出單一變數分配的緣故。

最後，**條件機率**是在屬於X的事件已經發生的前提之下，屬於Y的事件發生的機率，或是在屬於Y的事件已經發生的前提之下，屬於X的事件發生的機率。例如Pr(Y=i|X=j)是在X=j這個事件已經發生的前提下，Y=i這個事件發生的機率；而Pr(X=j|Y=i)是在Y=i這個事件已經發生的前提下，X=j這個事件發生的機率。

條件機率的樣本空間只是聯合機率樣本空間的一部份。在表一中，Y跟X聯合起來的樣本空間一共有IxJ個可能結果。但當我們以X=j這個事件已經發生為前提時，Y這個變數的樣本空間就被侷限在 {(1,j), (2,j), …, (i,j), …, (I,j)} 這I個結果的範圍裡。同樣的，當我們以Y= i這個事件已經發生為前提時，X這個變數的樣本空間就被侷限在 {(i,1), (i,2), …, (i,j), … (i,J)} 這J個結果的範圍裡。因為樣本空間改變，機率也會有所不同。其計算如下：

- ----廣告，請繼續往下閱讀-----

這也就是說，條件機率等於聯合機率除以條件變數的邊際機率。反過來講，聯合機率等於條件機率乘以條件變數的邊際機率，如下式所示：

此公式稱為機率的乘法法則（Multiplication Rule），這個法則對於理解貝式定理至關重要。

前述提及條件機率有兩種，分別為Pr(Y=i|X=j)以及Pr(X=j|Y=i)，差別僅在於是以X變數的特定事件為給定前提，還是以Y變數的特定事件為給定前提。表一中，因為X是「行」（column，台灣稱「行」，中國大陸稱「列」）的變數，我們把以X變數特定事件為給定前提的條件機率稱之為「行的條件機率」（column conditional probability）；如果是以Y變數特定事件為給定前提的條件機率，因為Y是「列」（row，台灣稱「列」，中國大陸稱「行」）的變數，我們稱之為「列的條件機率」（row conditional probability）。

Pr(Y=i|X=j)以及Pr(X=j|Y=i)這兩個機率，我們可以說它們互為「反機率」（inverse probability）。我們以X和Y分別只有兩個值為例，以表二和表三加以說明：

-