{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b575b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 初始化參數\n",
    "import os\n",
    "file_path = \"nstc_jobs_full.csv\"  # 爬蟲產出的 CSV 路徑\n",
    "model_name = \"all-MiniLM-L6-v2\"   # 向量化模型\n",
    "chunk_size = 300                   # 文件切割長度\n",
    "chunk_overlap = 30                 # 文件切割重疊\n",
    "encoding_batch_size = 32           # 向量化批次大小\n",
    "top_k = 5                          # 查詢回傳筆數\n",
    "save_path_prefix = \"faiss_recursive\"  # FAISS index 儲存前綴\n",
    "log_dir = \"logs\"                   # log 檔儲存資料夾\n",
    "output_log_csv = f\"{log_dir}/rag_results_log.csv\"\n",
    "output_log_excel = f\"{log_dir}/rag_results_log.xlsx\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ce7de123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_and_prepare_data(file_path):\n",
    "    \"\"\"讀取爬蟲 CSV 並產生文本與中繼資料。\n",
    "    參數: file_path (str) CSV 路徑\n",
    "    回傳: texts, metadatas\n",
    "    使用: texts, metas = load_and_prepare_data('nstc_jobs_full.csv')\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.dropna(subset=[\"職缺名稱\", \"發佈日期\", \"連結\", \"詳細內容\"])\n",
    "\n",
    "    texts = [\n",
    "        f\"職缺名稱：{row['職缺名稱']}\\n發佈日期：{row['發佈日期']}\\n詳細內容：\\n{row['詳細內容']}\"\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    "\n",
    "    metadatas = [\n",
    "        {\"職缺名稱\": row[\"職缺名稱\"], \"連結\": row[\"連結\"]}\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    "\n",
    "    return texts, metadatas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c194caeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def build_documents(texts, metadatas, chunk_size, chunk_overlap):\n",
    "    \"\"\"將文本依指定長度切割成文件片段。\n",
    "    texts: 原始文本列表\n",
    "    metadatas: 對應的中繼資料列表\n",
    "    chunk_size: 每段最大字元數\n",
    "    chunk_overlap: 重疊字元數\n",
    "    使用: docs = build_documents(texts, metadatas, 300, 30)\n",
    "    \"\"\"\n",
    "    docs = [Document(page_content=text, metadata=meta) for text, meta in zip(texts, metadatas)]\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2f72ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def hybrid_chunking(text, metadata, chunk_size=300, chunk_overlap=30):\n",
    "    \"\"\"依標題或空行分段後再切割過長段落。\n",
    "    text: 單篇文本\n",
    "    metadata: 此文本的中繼資料\n",
    "    chunk_size: 每段最大字元\n",
    "    chunk_overlap: 段落重疊量\n",
    "    使用: chunks = hybrid_chunking(texts[0], metadatas[0])\n",
    "    \"\"\"\n",
    "    # Step 1: 先依據 【標題】或雙換行或條列符號切段\n",
    "    segments = re.split(r\"(?=【[^】]+】)|(?<=\\n)\\d+\\.\\s+|\\n{2,}\", text)\n",
    "\n",
    "    # Step 2: 對每段做長度判斷，如太長則進一步切割\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    all_chunks = []\n",
    "\n",
    "    for seg in segments:\n",
    "        if len(seg.strip()) == 0:\n",
    "            continue\n",
    "        doc = Document(page_content=seg.strip(), metadata=metadata)\n",
    "        sub_chunks = splitter.split_documents([doc])\n",
    "        all_chunks.extend(sub_chunks)\n",
    "\n",
    "    return all_chunks\n",
    "def build_documents_hybrid(texts, metadatas, chunk_size=300, chunk_overlap=30):\n",
    "    \"\"\"對多筆文本進行 hybrid chunking，回傳所有切好的 Document 物件\"\"\"\n",
    "    all_docs = []\n",
    "    for text, meta in zip(texts, metadatas):\n",
    "        chunks = hybrid_chunking(text, meta, chunk_size, chunk_overlap)\n",
    "        all_docs.extend(chunks)\n",
    "    return all_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "sentence_para_chunking",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_documents_sentence(texts, metadatas):\n",
    "    '''將多篇文本以句子為單位切割。'''\n",
    "    all_docs = []\n",
    "    for text, meta in zip(texts, metadatas):\n",
    "        chunks = sentence_chunking(text, meta)\n",
    "        all_docs.extend(chunks)\n",
    "    return all_docs\n",
    "\n",
    "def build_documents_paragraph(texts, metadatas):\n",
    "    '''將多篇文本以段落為單位切割。'''\n",
    "    all_docs = []\n",
    "    for text, meta in zip(texts, metadatas):\n",
    "        chunks = paragraph_chunking(text, meta)\n",
    "        all_docs.extend(chunks)\n",
    "    return all_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8eb17894",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def build_faiss_index(split_docs, model_name, batch_size=32):\n",
    "    '''將文件片段編碼並建立 FAISS 索引。'''\n",
    "    model = SentenceTransformer(model_name)\n",
    "    contents = [doc.page_content for doc in split_docs]\n",
    "    embeddings = model.encode(contents, show_progress_bar=True, batch_size=batch_size)\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(np.array(embeddings))\n",
    "    return index, model, split_docs\n",
    "\n",
    "def load_or_build_index(docs, model_name, save_path_prefix, batch_size=32):\n",
    "    '''載入既有索引，或在不存在時重新建立並儲存。'''\n",
    "    index_file = f\"{save_path_prefix}.index\"\n",
    "    docs_file = f\"{save_path_prefix}_docs.pkl\"\n",
    "    if os.path.exists(index_file) and os.path.exists(docs_file):\n",
    "        print(f\"📦 載入已存在的 index：{save_path_prefix}\")\n",
    "        embedding_model = SentenceTransformer(model_name)\n",
    "        index, loaded_docs = load_faiss_index(save_path_prefix)\n",
    "        return index, embedding_model, loaded_docs\n",
    "    else:\n",
    "        print(f\"🛠️ 建立新 index 並儲存至：{save_path_prefix}\")\n",
    "        index, embedding_model, split_docs = build_faiss_index(docs, model_name, batch_size)\n",
    "        save_faiss_index(index, split_docs, save_path_prefix)\n",
    "        return index, embedding_model, split_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f761d6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "import faiss\n",
    "\n",
    "def save_faiss_index(index, docs, save_path_prefix='faiss_index'):\n",
    "    '''將索引及文件序列化存檔。'''\n",
    "    faiss.write_index(index, f\"{save_path_prefix}.index\")\n",
    "    with open(f\"{save_path_prefix}_docs.pkl\", 'wb') as f:\n",
    "        pickle.dump(docs, f)\n",
    "    print(f\"✅ 已儲存 FAISS index 和文件到：{save_path_prefix}.index / _docs.pkl\")\n",
    "\n",
    "def load_faiss_index(save_path_prefix='faiss_index'):\n",
    "    '''讀取先前儲存的索引與文件。'''\n",
    "    index = faiss.read_index(f\"{save_path_prefix}.index\")\n",
    "    with open(f\"{save_path_prefix}_docs.pkl\", 'rb') as f:\n",
    "        docs = pickle.load(f)\n",
    "    print(f\"📂 成功載入 index 與 documents，筆數：{len(docs)}\")\n",
    "    return index, docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "46c3f6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 舊版 rag_query，已被新版取代\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a2d388e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def save_rag_log(results, model_name, chunk_size, chunk_overlap, save_path=\"rag_results_log.csv\"):\n",
    "    \"\"\"將查詢結果寫入 CSV 檔。\n",
    "    results: rag_query 產生的結果\n",
    "    model_name: 使用的模型名稱\n",
    "    chunk_size: 切割長度設定\n",
    "    chunk_overlap: 重疊長度設定\n",
    "    save_path: 儲存的 CSV 路徑\n",
    "    使用: save_rag_log(results, model_name, chunk_size, chunk_overlap)\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    write_header = not os.path.exists(save_path)\n",
    "\n",
    "    with open(save_path, \"a\", newline='', encoding=\"utf-8-sig\") as f:  # 👈 修正編碼為 utf-8-sig\n",
    "        writer = csv.DictWriter(\n",
    "            f,\n",
    "            fieldnames=[\"timestamp\", \"query\", \"rank\", \"職缺名稱\", \"連結\", \"摘要\", \"model_name\", \"chunk_size\", \"chunk_overlap\"]\n",
    "        )\n",
    "        if write_header:\n",
    "            writer.writeheader()\n",
    "        for row in results:\n",
    "            writer.writerow({\n",
    "                \"timestamp\": timestamp,\n",
    "                \"query\": row[\"query\"],\n",
    "                \"rank\": row[\"rank\"],\n",
    "                \"職缺名稱\": row[\"職缺名稱\"],\n",
    "                \"連結\": row[\"連結\"],\n",
    "                \"摘要\": row[\"摘要\"],\n",
    "                \"model_name\": model_name,\n",
    "                \"chunk_size\": chunk_size,\n",
    "                \"chunk_overlap\": chunk_overlap\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "56ebd91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def save_rag_log_to_excel(results, model_name, chunk_size, chunk_overlap, chunking_strategy, save_path=\"rag_results_log.xlsx\"):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # 將本次查詢結果轉成 DataFrame\n",
    "    new_df = pd.DataFrame([{\n",
    "        \"timestamp\": timestamp,\n",
    "        \"query\": row[\"query\"],\n",
    "        \"rank\": row[\"rank\"],\n",
    "        \"職缺名稱\": row[\"職缺名稱\"],\n",
    "        \"連結\": row[\"連結\"],\n",
    "        \"摘要\": row[\"摘要\"],\n",
    "        \"model_name\": model_name,\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"chunk_overlap\": chunk_overlap,\n",
    "        \"chunking_strategy\": chunking_strategy\n",
    "    } for row in results])\n",
    "\n",
    "    # 如果檔案已存在就讀入合併\n",
    "    if os.path.exists(save_path):\n",
    "        old_df = pd.read_excel(save_path)\n",
    "        combined_df = pd.concat([old_df, new_df], ignore_index=True)\n",
    "    else:\n",
    "        combined_df = new_df\n",
    "\n",
    "    # 寫入 Excel\n",
    "    combined_df.to_excel(save_path, index=False)\n",
    "    print(f\"✅ 已寫入 {len(new_df)} 筆紀錄，累計：{len(combined_df)} 筆 ➜ {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d48af35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        def rag_query(query, top_k, embedding_model, index, indexed_docs):\n",
    "            '''根據查詢字串在索引中取回相關片段。'''\n",
    "            query_embedding = embedding_model.encode([query])\n",
    "            D, I = index.search(np.array(query_embedding), top_k)\n",
    "            results = []\n",
    "            for i, idx in enumerate(I[0]):\n",
    "                if idx >= len(indexed_docs):\n",
    "                    print(f\"⚠️ 無效索引：{idx} 超出 indexed_docs 範圍（{len(indexed_docs)}）\")\n",
    "                    continue\n",
    "                doc = indexed_docs[idx]\n",
    "                snippet = doc.page_content.strip().replace('\n",
    "', ' ')\n",
    "                if len(snippet) > 200:\n",
    "                    snippet = snippet[:200] + '...'\n",
    "                results.append({'query': query,\n",
    "                                'rank': i + 1,\n",
    "                                '職缺名稱': doc.metadata['職缺名稱'],\n",
    "                                '連結': doc.metadata['連結'],\n",
    "                                '摘要': snippet})\n",
    "            return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c793682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 參數設定已在最上方定義\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "345700e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此區塊已整合至主流程\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8b31ab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此區塊已整合至主流程\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a4cb9244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此區塊已整合至主流程\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c48163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此區塊已整合至主流程\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a424482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此區塊已整合至主流程\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "77d2052f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此區塊已整合至主流程\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "97cacc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此區塊已整合至主流程\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cc717ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此區塊已整合至主流程\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a2efce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 主流程執行 ===\n",
    "texts, metadatas = load_and_prepare_data(file_path)\n",
    "docs = build_documents_hybrid(texts, metadatas, chunk_size, chunk_overlap)\n",
    "index, embedding_model, indexed_docs = load_or_build_index(\n",
    "    docs,\n",
    "    model_name=model_name,\n",
    "    save_path_prefix=save_path_prefix,\n",
    "    batch_size=encoding_batch_size,\n",
    ")\n",
    "print(f\"Hybrid Chunking 完成，共產生 {len(docs)} 個 chunks\")\n",
    "\n",
    "query = \"生物相關的職缺有哪些\"\n",
    "results = rag_query(query, top_k, embedding_model, index, indexed_docs)\n",
    "\n",
    "save_rag_log_to_excel(\n",
    "    results,\n",
    "    model_name=model_name,\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    chunking_strategy='hybrid_chunking',\n",
    "    save_path=output_log_excel,\n",
    ")\n",
    "save_rag_log(\n",
    "    results,\n",
    "    model_name=model_name,\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    save_path=output_log_csv,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}