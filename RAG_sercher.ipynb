{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b575b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === åƒæ•¸è¨­å®š ===\n",
    "file_path = \"nstc_jobs_full.csv\"\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "chunk_size = 300\n",
    "chunk_overlap = 30\n",
    "encoding_batch_size = 32\n",
    "top_k = 5\n",
    "output_log_path = \"rag_results_log.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce7de123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_and_prepare_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.dropna(subset=[\"è·ç¼ºåç¨±\", \"ç™¼ä½ˆæ—¥æœŸ\", \"é€£çµ\", \"è©³ç´°å…§å®¹\"])\n",
    "\n",
    "    texts = [\n",
    "        f\"è·ç¼ºåç¨±ï¼š{row['è·ç¼ºåç¨±']}\\nç™¼ä½ˆæ—¥æœŸï¼š{row['ç™¼ä½ˆæ—¥æœŸ']}\\nè©³ç´°å…§å®¹ï¼š\\n{row['è©³ç´°å…§å®¹']}\"\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    "\n",
    "    metadatas = [\n",
    "        {\"è·ç¼ºåç¨±\": row[\"è·ç¼ºåç¨±\"], \"é€£çµ\": row[\"é€£çµ\"]}\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    "\n",
    "    return texts, metadatas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c194caeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def build_documents(texts, metadatas, chunk_size, chunk_overlap):\n",
    "    docs = [Document(page_content=text, metadata=meta) for text, meta in zip(texts, metadatas)]\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f72ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def hybrid_chunking(text, metadata, chunk_size=300, chunk_overlap=30):\n",
    "    # Step 1: å…ˆä¾æ“š ã€æ¨™é¡Œã€‘æˆ–é›™æ›è¡Œæˆ–æ¢åˆ—ç¬¦è™Ÿåˆ‡æ®µ\n",
    "    segments = re.split(r\"(?=ã€[^ã€‘]+ã€‘)|(?<=\\n)\\d+\\.\\s+|\\n{2,}\", text)\n",
    "\n",
    "    # Step 2: å°æ¯æ®µåšé•·åº¦åˆ¤æ–·ï¼Œå¦‚å¤ªé•·å‰‡é€²ä¸€æ­¥åˆ‡å‰²\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    all_chunks = []\n",
    "\n",
    "    for seg in segments:\n",
    "        if len(seg.strip()) == 0:\n",
    "            continue\n",
    "        doc = Document(page_content=seg.strip(), metadata=metadata)\n",
    "        sub_chunks = splitter.split_documents([doc])\n",
    "        all_chunks.extend(sub_chunks)\n",
    "\n",
    "    return all_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "def sentence_chunking(text, metadata):\n",
    "    \"Split text into sentences when punctuation is clear.\"\n",
    "    import re\n",
    "    sentences = re.split(r'[ã€‚.!?]\\s*', text)\n",
    "    return [Document(page_content=s.strip(), metadata=metadata) for s in sentences if s.strip()]\n",
    "\n",
    "def paragraph_chunking(text, metadata):\n",
    "    \"Split text into paragraphs separated by blank lines.\"\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "    return [Document(page_content=p, metadata=metadata) for p in paragraphs]\n"
   ],
   "id": "sentence_para_chunking"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eb17894",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "def build_faiss_index(split_docs, model_name, batch_size=32):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    contents = [doc.page_content for doc in split_docs]\n",
    "    embeddings = model.encode(contents, show_progress_bar=True, batch_size=batch_size)\n",
    "\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(np.array(embeddings))\n",
    "\n",
    "    return index, model, split_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46c3f6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(query, top_k, embedding_model, index, indexed_docs):\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    D, I = index.search(np.array(query_embedding), top_k)\n",
    "\n",
    "    results = []\n",
    "    for i, idx in enumerate(I[0]):\n",
    "        doc = indexed_docs[idx]\n",
    "        snippet = doc.page_content.strip().replace(\"\\n\", \" \")\n",
    "        if len(snippet) > 200:\n",
    "            snippet = snippet[:200] + \"...\"\n",
    "\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"rank\": i + 1,\n",
    "            \"è·ç¼ºåç¨±\": doc.metadata[\"è·ç¼ºåç¨±\"],\n",
    "            \"é€£çµ\": doc.metadata[\"é€£çµ\"],\n",
    "            \"æ‘˜è¦\": snippet\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2d388e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def save_rag_log(results, model_name, chunk_size, chunk_overlap, save_path=\"rag_results_log.csv\"):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    write_header = not os.path.exists(save_path)\n",
    "\n",
    "    with open(save_path, \"a\", newline='', encoding=\"utf-8-sig\") as f:  # ğŸ‘ˆ ä¿®æ­£ç·¨ç¢¼ç‚º utf-8-sig\n",
    "        writer = csv.DictWriter(\n",
    "            f,\n",
    "            fieldnames=[\"timestamp\", \"query\", \"rank\", \"è·ç¼ºåç¨±\", \"é€£çµ\", \"æ‘˜è¦\", \"model_name\", \"chunk_size\", \"chunk_overlap\"]\n",
    "        )\n",
    "        if write_header:\n",
    "            writer.writeheader()\n",
    "        for row in results:\n",
    "            writer.writerow({\n",
    "                \"timestamp\": timestamp,\n",
    "                \"query\": row[\"query\"],\n",
    "                \"rank\": row[\"rank\"],\n",
    "                \"è·ç¼ºåç¨±\": row[\"è·ç¼ºåç¨±\"],\n",
    "                \"é€£çµ\": row[\"é€£çµ\"],\n",
    "                \"æ‘˜è¦\": row[\"æ‘˜è¦\"],\n",
    "                \"model_name\": model_name,\n",
    "                \"chunk_size\": chunk_size,\n",
    "                \"chunk_overlap\": chunk_overlap\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56ebd91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def save_rag_log_to_excel(results, model_name, chunk_size, chunk_overlap, save_path=\"rag_results_log.xlsx\"):\n",
    "    from datetime import datetime\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    df = pd.DataFrame([{\n",
    "        \"timestamp\": timestamp,\n",
    "        \"query\": row[\"query\"],\n",
    "        \"rank\": row[\"rank\"],\n",
    "        \"è·ç¼ºåç¨±\": row[\"è·ç¼ºåç¨±\"],\n",
    "        \"é€£çµ\": row[\"é€£çµ\"],\n",
    "        \"æ‘˜è¦\": row[\"æ‘˜è¦\"],\n",
    "        \"model_name\": model_name,\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"chunk_overlap\": chunk_overlap\n",
    "    } for row in results])\n",
    "\n",
    "    df.to_excel(save_path, index=False)  # â— Excel ä¸ç”¨æ“”å¿ƒç·¨ç¢¼å•é¡Œ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "345700e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 48/48 [00:54<00:00,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ ä¸€æ¬¡æ€§åˆå§‹åŒ–ç³»çµ±\n",
    "texts, metadatas = load_and_prepare_data(file_path)\n",
    "split_docs = build_documents(texts, metadatas, chunk_size, chunk_overlap)\n",
    "index, embedding_model, indexed_docs = build_faiss_index(split_docs, model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65c18bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” 1. åœ‹ç«‹ä¸­å±±å¤§å­¸æ–°æµ·ç ”3è™Ÿè²´é‡å„€å™¨ä½¿ç”¨ä¸­å¿ƒèª å¾µå°ˆä»»æŠ€è¡“å“¡1å\n",
      "ğŸ”— https://www.nstc.gov.tw/folksonomy/detail/ddc2e921-92c5-4004-8c2f-be2373c53f52?l=ch\n",
      "ğŸ“ ç›¸é—œæ‡‰å¾µè³‡æ–™äºˆä»¥ä¿å¯†ï¼Œåˆè€…ç´„è«‡ï¼Œä¸åˆè€…æ•ä¸å¦è¡Œé€šçŸ¥ã€‚ ç™¼ä½ˆæ—¥æœŸï¼š2025-07-04 00:00:00\n",
      "\n",
      "ğŸ” 2. [å¾µæ‰] åœ‹ç«‹è‡ºç£å¤§å­¸é˜²ç½æ¸›å®³èˆ‡éŸŒæ€§å­¸ç¨‹ (ç¶ â€§éŸŒæ€§ç ”ç©¶å®¤) å¾µæ±‚éƒ½å¸‚è¦åŠƒ/æ™¯è§€/åœ°ç†è³‡è¨Šå°ˆé•· [å°ˆä»»è¨ˆç•«åŠ©ç†]\n",
      "ğŸ”— https://www.nstc.gov.tw/folksonomy/detail/2793c7ef-b68d-4f00-9388-e011b78b9553?l=ch\n",
      "ğŸ“ 3.å…¶ä»–æœ‰åˆ©ç”³è«‹ä¹‹ç›¸é—œæ–‡ä»¶ ç™¼ä½ˆæ—¥æœŸï¼š2025-07-21 00:00:00\n",
      "\n",
      "ğŸ” 3. ä¸­åœ‹é†«è—¥å¤§å­¸ ç™Œç—‡ç”Ÿç‰©ç²¾æº–é†«å­¸ç ”ç©¶ä¸­å¿ƒ  ç‹ç´¹æ¤¿è€å¸«å¯¦é©—å®¤ èª å¾µ åšå£«å¾Œç ”ç©¶å“¡\n",
      "ğŸ”— https://www.nstc.gov.tw/folksonomy/detail/701ca4f1-a9f5-4a61-9b66-c4cf60f5c093?l=ch\n",
      "ğŸ“ æ­¡è¿å°ç™Œç—‡ç ”ç©¶æœ‰èˆˆè¶£çš„å¤¥ä¼´åŠ å…¥æˆ‘å€‘çš„åœ˜éšŠï¼ ç™¼ä½ˆæ—¥æœŸï¼š2025-07-14 00:00:00\n",
      "\n",
      "ğŸ” 4. ä¸­åœ‹é†«è—¥å¤§å­¸ ç™Œç—‡ç”Ÿç‰©ç²¾æº–é†«å­¸ç ”ç©¶ä¸­å¿ƒ  ç‹ç´¹æ¤¿è€å¸«å¯¦é©—å®¤ èª å¾µ ç¢©å£«ç´šç ”ç©¶åŠ©ç†\n",
      "ğŸ”— https://www.nstc.gov.tw/folksonomy/detail/2521ae27-55c0-4f27-9ded-b4bc908c1aff?l=ch\n",
      "ğŸ“ æ­¡è¿å°ç™Œç—‡ç ”ç©¶æœ‰èˆˆè¶£çš„å¤¥ä¼´åŠ å…¥æˆ‘å€‘çš„åœ˜éšŠï¼ ç™¼ä½ˆæ—¥æœŸï¼š2025-07-14 00:00:00\n",
      "\n",
      "ğŸ” 5. åœ‹ç«‹è‡ºæ±å¤§å­¸é€šè­˜æ•™è‚²ä¸­å¿ƒå¾µè˜å°ˆä»»åŠ©ç†æ•™æˆä»¥ä¸Šæ•™å¸«å¾µæ‰å…¬å‘Šï¼Œæ”¶ä»¶è‡³114å¹´8æœˆ15æ—¥æ­¢ã€‚\n",
      "ğŸ”— https://www.nstc.gov.tw/folksonomy/detail/e407fdbc-62c9-4e09-b08a-35a897cc4186?l=ch\n",
      "ğŸ“ å…¶    å®ƒï¼š ç›¸é—œè¨Šæ¯ï¼Œè«‹è‡³æœ¬æ ¡é¦–é å¾µäººå•Ÿäº‹https://psn.nttu.edu.tw/p/406-1047-165359,r595.php?Lang=zh-twæŸ¥è©¢ä¸‹è¼‰ã€‚ è¯çµ¡äººå§“å: æå®¶å©•å°å§ è¯çµ¡äººé›»è©±: 089-517492 é›»å­ä¿¡ç®±ï¼ševalee@nttu.edu.tw ç™¼ä½ˆæ—¥æœŸï¼š2025-07-09 00:00:00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ æŸ¥è©¢ä¸¦å„²å­˜ç´€éŒ„\n",
    "query = \"ææ–™ç›¸é—œçš„è·ç¼ºæœ‰å“ªäº›ï¼Ÿ\"\n",
    "results = rag_query(query, top_k, embedding_model, index, indexed_docs)\n",
    "save_rag_log(results, model_name, chunk_size, chunk_overlap, save_path=output_log_path)\n",
    "save_rag_log_to_excel(results, model_name, chunk_size, chunk_overlap, save_path=\"rag_results_log.xlsx\")\n",
    "# ğŸ‘€ é¡¯ç¤ºæŸ¥è©¢çµæœ\n",
    "for r in results:\n",
    "    print(f\"ğŸ” {r['rank']}. {r['è·ç¼ºåç¨±']}\")\n",
    "    print(f\"ğŸ”— {r['é€£çµ']}\")\n",
    "    print(f\"ğŸ“ {r['æ‘˜è¦']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80f2065a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_documents_hybrid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m docs_recursive \u001b[38;5;241m=\u001b[39m build_documents(texts, metadatas, chunk_size, chunk_overlap)  \u001b[38;5;66;03m# åŸæ³•\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m docs_hybrid \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_documents_hybrid\u001b[49m(texts, metadatas, chunk_size, chunk_overlap)  \u001b[38;5;66;03m# æ”¹ç”¨ hybrid chunking\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'build_documents_hybrid' is not defined"
     ]
    }
   ],
   "source": [
    "docs_recursive = build_documents(texts, metadatas, chunk_size, chunk_overlap)  # åŸæ³•\n",
    "docs_hybrid = build_documents_hybrid(texts, metadatas, chunk_size, chunk_overlap)  # æ”¹ç”¨ hybrid chunking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b31ab79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}