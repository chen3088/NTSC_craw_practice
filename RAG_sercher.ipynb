{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b575b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ce7de123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_and_prepare_data(file_path):\n",
    "    \"\"\"è®€å–çˆ¬èŸ² CSV ä¸¦ç”¢ç”Ÿæ–‡æœ¬èˆ‡ä¸­ç¹¼è³‡æ–™ã€‚\n",
    "    åƒæ•¸: file_path (str) CSV è·¯å¾‘\n",
    "    å›å‚³: texts, metadatas\n",
    "    ä½¿ç”¨: texts, metas = load_and_prepare_data('nstc_jobs_full.csv')\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.dropna(subset=[\"è·ç¼ºåç¨±\", \"ç™¼ä½ˆæ—¥æœŸ\", \"é€£çµ\", \"è©³ç´°å…§å®¹\"])\n",
    "\n",
    "    texts = [\n",
    "        f\"è·ç¼ºåç¨±ï¼š{row['è·ç¼ºåç¨±']}\\nç™¼ä½ˆæ—¥æœŸï¼š{row['ç™¼ä½ˆæ—¥æœŸ']}\\nè©³ç´°å…§å®¹ï¼š\\n{row['è©³ç´°å…§å®¹']}\"\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    "\n",
    "    metadatas = [\n",
    "        {\"è·ç¼ºåç¨±\": row[\"è·ç¼ºåç¨±\"], \"é€£çµ\": row[\"é€£çµ\"]}\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    "\n",
    "    return texts, metadatas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c194caeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def build_documents(texts, metadatas, chunk_size, chunk_overlap):\n",
    "    \"\"\"å°‡æ–‡æœ¬ä¾æŒ‡å®šé•·åº¦åˆ‡å‰²æˆæ–‡ä»¶ç‰‡æ®µã€‚\n",
    "    texts: åŸå§‹æ–‡æœ¬åˆ—è¡¨\n",
    "    metadatas: å°æ‡‰çš„ä¸­ç¹¼è³‡æ–™åˆ—è¡¨\n",
    "    chunk_size: æ¯æ®µæœ€å¤§å­—å…ƒæ•¸\n",
    "    chunk_overlap: é‡ç–Šå­—å…ƒæ•¸\n",
    "    ä½¿ç”¨: docs = build_documents(texts, metadatas, 300, 30)\n",
    "    \"\"\"\n",
    "    docs = [Document(page_content=text, metadata=meta) for text, meta in zip(texts, metadatas)]\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2f72ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def hybrid_chunking(text, metadata, chunk_size=300, chunk_overlap=30):\n",
    "    \"\"\"ä¾æ¨™é¡Œæˆ–ç©ºè¡Œåˆ†æ®µå¾Œå†åˆ‡å‰²éé•·æ®µè½ã€‚\n",
    "    text: å–®ç¯‡æ–‡æœ¬\n",
    "    metadata: æ­¤æ–‡æœ¬çš„ä¸­ç¹¼è³‡æ–™\n",
    "    chunk_size: æ¯æ®µæœ€å¤§å­—å…ƒ\n",
    "    chunk_overlap: æ®µè½é‡ç–Šé‡\n",
    "    ä½¿ç”¨: chunks = hybrid_chunking(texts[0], metadatas[0])\n",
    "    \"\"\"\n",
    "    # Step 1: å…ˆä¾æ“š ã€æ¨™é¡Œã€‘æˆ–é›™æ›è¡Œæˆ–æ¢åˆ—ç¬¦è™Ÿåˆ‡æ®µ\n",
    "    segments = re.split(r\"(?=ã€[^ã€‘]+ã€‘)|(?<=\\n)\\d+\\.\\s+|\\n{2,}\", text)\n",
    "\n",
    "    # Step 2: å°æ¯æ®µåšé•·åº¦åˆ¤æ–·ï¼Œå¦‚å¤ªé•·å‰‡é€²ä¸€æ­¥åˆ‡å‰²\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    all_chunks = []\n",
    "\n",
    "    for seg in segments:\n",
    "        if len(seg.strip()) == 0:\n",
    "            continue\n",
    "        doc = Document(page_content=seg.strip(), metadata=metadata)\n",
    "        sub_chunks = splitter.split_documents([doc])\n",
    "        all_chunks.extend(sub_chunks)\n",
    "\n",
    "    return all_chunks\n",
    "def build_documents_hybrid(texts, metadatas, chunk_size=300, chunk_overlap=30):\n",
    "    \"\"\"å°å¤šç­†æ–‡æœ¬é€²è¡Œ hybrid chunkingï¼Œå›å‚³æ‰€æœ‰åˆ‡å¥½çš„ Document ç‰©ä»¶\"\"\"\n",
    "    all_docs = []\n",
    "    for text, meta in zip(texts, metadatas):\n",
    "        chunks = hybrid_chunking(text, meta, chunk_size, chunk_overlap)\n",
    "        all_docs.extend(chunks)\n",
    "    return all_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "sentence_para_chunking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_documents_sentence(texts, metadatas):\n",
    "    all_docs = []\n",
    "    for text, meta in zip(texts, metadatas):\n",
    "        chunks = sentence_chunking(text, meta)\n",
    "        all_docs.extend(chunks)\n",
    "    return all_docs\n",
    "def build_documents_paragraph(texts, metadatas):\n",
    "    all_docs = []\n",
    "    for text, meta in zip(texts, metadatas):\n",
    "        chunks = paragraph_chunking(text, meta)\n",
    "        all_docs.extend(chunks)\n",
    "    return all_docs\n",
    "def build_documents_sentence(texts, metadatas):\n",
    "    all_docs = []\n",
    "    for text, meta in zip(texts, metadatas):\n",
    "        chunks = sentence_chunking(text, meta)\n",
    "        all_docs.extend(chunks)\n",
    "    return all_docs\n",
    "def build_documents_paragraph(texts, metadatas):\n",
    "    all_docs = []\n",
    "    for text, meta in zip(texts, metadatas):\n",
    "        chunks = paragraph_chunking(text, meta)\n",
    "        all_docs.extend(chunks)\n",
    "    return all_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8eb17894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "def build_faiss_index(split_docs, model_name, batch_size=32):\n",
    "    \"\"\"å–å¾—å‘é‡ä¸¦å»ºç«‹ FAISS ç´¢å¼•ã€‚\n",
    "    split_docs: Document ç‰‡æ®µåˆ—è¡¨\n",
    "    model_name: åµŒå…¥æ¨¡å‹åç¨±\n",
    "    batch_size: æ‰¹æ¬¡è™•ç†é‡\n",
    "    ä½¿ç”¨: index, model, docs = build_faiss_index(split_docs, 'all-MiniLM-L6-v2')\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    contents = [doc.page_content for doc in split_docs]\n",
    "    embeddings = model.encode(contents, show_progress_bar=True, batch_size=batch_size)\n",
    "\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(np.array(embeddings))\n",
    "\n",
    "    return index, model, split_docs\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def load_or_build_index(docs, model_name, save_path_prefix, batch_size=32):\n",
    "    index_file = f\"{save_path_prefix}.index\"\n",
    "    docs_file = f\"{save_path_prefix}_docs.pkl\"\n",
    "\n",
    "    if os.path.exists(index_file) and os.path.exists(docs_file):\n",
    "        print(f\"ğŸ“¦ è¼‰å…¥å·²å­˜åœ¨çš„ indexï¼š{save_path_prefix}\")\n",
    "        embedding_model = SentenceTransformer(model_name)\n",
    "        index, loaded_docs = load_faiss_index(save_path_prefix)\n",
    "        return index, embedding_model, loaded_docs\n",
    "    else:\n",
    "        print(f\"ğŸ› ï¸ å»ºç«‹æ–° index ä¸¦å„²å­˜è‡³ï¼š{save_path_prefix}\")\n",
    "        index, embedding_model, split_docs = build_faiss_index(docs, model_name, batch_size)\n",
    "        save_faiss_index(index, split_docs, save_path_prefix)\n",
    "        return index, embedding_model, split_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f761d6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_faiss_index(index, docs, save_path_prefix=\"faiss_index\"):\n",
    "    # å„²å­˜ FAISS index æœ¬é«” (.index)\n",
    "    faiss.write_index(index, f\"{save_path_prefix}.index\")\n",
    "    \n",
    "    # å„²å­˜å°æ‡‰çš„åŸå§‹ documents (.pkl)\n",
    "    with open(f\"{save_path_prefix}_docs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(docs, f)\n",
    "\n",
    "    print(f\"âœ… å·²å„²å­˜ FAISS index å’Œæ–‡ä»¶åˆ°ï¼š{save_path_prefix}.index / _docs.pkl\")\n",
    "def load_faiss_index(save_path_prefix=\"faiss_index\"):\n",
    "    # è®€å– FAISS index\n",
    "    index = faiss.read_index(f\"{save_path_prefix}.index\")\n",
    "    \n",
    "    # è®€å–åŸå§‹æ–‡ä»¶ chunks\n",
    "    with open(f\"{save_path_prefix}_docs.pkl\", \"rb\") as f:\n",
    "        docs = pickle.load(f)\n",
    "\n",
    "    print(f\"ğŸ“‚ æˆåŠŸè¼‰å…¥ index èˆ‡ documentsï¼Œç­†æ•¸ï¼š{len(docs)}\")\n",
    "    return index, docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "46c3f6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(query, top_k, embedding_model, index, indexed_docs):\n",
    "    \"\"\"åœ¨å‘é‡ç´¢å¼•ä¸­å–å¾—ç›¸é—œç‰‡æ®µã€‚\n",
    "    query: æŸ¥è©¢å­—ä¸²\n",
    "    top_k: å›å‚³å‰å¹¾ç­†çµæœ\n",
    "    embedding_model: ç”¨ä¾†ç·¨ç¢¼æŸ¥è©¢çš„æ¨¡å‹\n",
    "    index: FAISS ç´¢å¼•\n",
    "    indexed_docs: èˆ‡ç´¢å¼•å°æ‡‰çš„æ–‡ä»¶ç‰‡æ®µ\n",
    "    ä½¿ç”¨: results = rag_query('é—œéµå­—', 5, model, index, docs)\n",
    "    \"\"\"\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    D, I = index.search(np.array(query_embedding), top_k)\n",
    "\n",
    "    results = []\n",
    "    for i, idx in enumerate(I[0]):\n",
    "        doc = indexed_docs[idx]\n",
    "        snippet = doc.page_content.strip().replace(\"\\n\", \" \")\n",
    "        if len(snippet) > 200:\n",
    "            snippet = snippet[:200] + \"...\"\n",
    "\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"rank\": i + 1,\n",
    "            \"è·ç¼ºåç¨±\": doc.metadata[\"è·ç¼ºåç¨±\"],\n",
    "            \"é€£çµ\": doc.metadata[\"é€£çµ\"],\n",
    "            \"æ‘˜è¦\": snippet\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a2d388e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def save_rag_log(results, model_name, chunk_size, chunk_overlap, save_path=\"rag_results_log.csv\"):\n",
    "    \"\"\"å°‡æŸ¥è©¢çµæœå¯«å…¥ CSV æª”ã€‚\n",
    "    results: rag_query ç”¢ç”Ÿçš„çµæœ\n",
    "    model_name: ä½¿ç”¨çš„æ¨¡å‹åç¨±\n",
    "    chunk_size: åˆ‡å‰²é•·åº¦è¨­å®š\n",
    "    chunk_overlap: é‡ç–Šé•·åº¦è¨­å®š\n",
    "    save_path: å„²å­˜çš„ CSV è·¯å¾‘\n",
    "    ä½¿ç”¨: save_rag_log(results, model_name, chunk_size, chunk_overlap)\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    write_header = not os.path.exists(save_path)\n",
    "\n",
    "    with open(save_path, \"a\", newline='', encoding=\"utf-8-sig\") as f:  # ğŸ‘ˆ ä¿®æ­£ç·¨ç¢¼ç‚º utf-8-sig\n",
    "        writer = csv.DictWriter(\n",
    "            f,\n",
    "            fieldnames=[\"timestamp\", \"query\", \"rank\", \"è·ç¼ºåç¨±\", \"é€£çµ\", \"æ‘˜è¦\", \"model_name\", \"chunk_size\", \"chunk_overlap\"]\n",
    "        )\n",
    "        if write_header:\n",
    "            writer.writeheader()\n",
    "        for row in results:\n",
    "            writer.writerow({\n",
    "                \"timestamp\": timestamp,\n",
    "                \"query\": row[\"query\"],\n",
    "                \"rank\": row[\"rank\"],\n",
    "                \"è·ç¼ºåç¨±\": row[\"è·ç¼ºåç¨±\"],\n",
    "                \"é€£çµ\": row[\"é€£çµ\"],\n",
    "                \"æ‘˜è¦\": row[\"æ‘˜è¦\"],\n",
    "                \"model_name\": model_name,\n",
    "                \"chunk_size\": chunk_size,\n",
    "                \"chunk_overlap\": chunk_overlap\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "56ebd91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def save_rag_log_to_excel(results, model_name, chunk_size, chunk_overlap, chunking_strategy, save_path=\"rag_results_log.xlsx\"):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # å°‡æœ¬æ¬¡æŸ¥è©¢çµæœè½‰æˆ DataFrame\n",
    "    new_df = pd.DataFrame([{\n",
    "        \"timestamp\": timestamp,\n",
    "        \"query\": row[\"query\"],\n",
    "        \"rank\": row[\"rank\"],\n",
    "        \"è·ç¼ºåç¨±\": row[\"è·ç¼ºåç¨±\"],\n",
    "        \"é€£çµ\": row[\"é€£çµ\"],\n",
    "        \"æ‘˜è¦\": row[\"æ‘˜è¦\"],\n",
    "        \"model_name\": model_name,\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"chunk_overlap\": chunk_overlap,\n",
    "        \"chunking_strategy\": chunking_strategy\n",
    "    } for row in results])\n",
    "\n",
    "    # å¦‚æœæª”æ¡ˆå·²å­˜åœ¨å°±è®€å…¥åˆä½µ\n",
    "    if os.path.exists(save_path):\n",
    "        old_df = pd.read_excel(save_path)\n",
    "        combined_df = pd.concat([old_df, new_df], ignore_index=True)\n",
    "    else:\n",
    "        combined_df = new_df\n",
    "\n",
    "    # å¯«å…¥ Excel\n",
    "    combined_df.to_excel(save_path, index=False)\n",
    "    print(f\"âœ… å·²å¯«å…¥ {len(new_df)} ç­†ç´€éŒ„ï¼Œç´¯è¨ˆï¼š{len(combined_df)} ç­† âœ {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d48af35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(query, top_k, embedding_model, index, indexed_docs):\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    D, I = index.search(np.array(query_embedding), top_k)\n",
    "\n",
    "    results = []\n",
    "    for i, idx in enumerate(I[0]):\n",
    "        if idx >= len(indexed_docs):\n",
    "            print(f\"âš ï¸ ç„¡æ•ˆç´¢å¼•ï¼š{idx} è¶…å‡º indexed_docs ç¯„åœï¼ˆ{len(indexed_docs)}ï¼‰\")\n",
    "            continue  # è·³ééŒ¯èª¤ç´¢å¼•\n",
    "\n",
    "        doc = indexed_docs[idx]\n",
    "        snippet = doc.page_content.strip().replace(\"\\n\", \" \")\n",
    "        if len(snippet) > 200:\n",
    "            snippet = snippet[:200] + \"...\"\n",
    "\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"rank\": i + 1,\n",
    "            \"è·ç¼ºåç¨±\": doc.metadata[\"è·ç¼ºåç¨±\"],\n",
    "            \"é€£çµ\": doc.metadata[\"é€£çµ\"],\n",
    "            \"æ‘˜è¦\": snippet\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c793682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === åƒæ•¸è¨­å®š ===\n",
    "file_path = \"nstc_jobs_full.csv\"\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "chunk_size = 300\n",
    "chunk_overlap = 30\n",
    "encoding_batch_size = 32\n",
    "top_k = 5\n",
    "output_log_path = \"rag_results_log.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "345700e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ è¼‰å…¥å·²å­˜åœ¨çš„ indexï¼šfaiss_recursive\n",
      "ğŸ“‚ æˆåŠŸè¼‰å…¥ index èˆ‡ documentsï¼Œç­†æ•¸ï¼š3146\n",
      "Hybrid Chunking å®Œæˆï¼Œå…±ç”¢ç”Ÿ 3146 å€‹ chunks\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ ä¸€æ¬¡æ€§åˆå§‹åŒ–ç³»çµ±\n",
    "texts, metadatas = load_and_prepare_data(file_path)\n",
    "docs_hybrid = build_documents_hybrid(texts, metadatas, chunk_size, chunk_overlap)\n",
    "\n",
    "index, embedding_model, indexed_docs = load_or_build_index(\n",
    "    docs_hybrid, model_name=\"all-MiniLM-L6-v2\", save_path_prefix=\"faiss_recursive\"\n",
    ")\n",
    "print(f\"Hybrid Chunking å®Œæˆï¼Œå…±ç”¢ç”Ÿ {len(docs_hybrid)} å€‹ chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8b31ab79",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"ç”Ÿç‰©ç›¸é—œçš„è·ç¼ºæœ‰å“ªäº›\"\n",
    "results = rag_query(query, top_k, embedding_model, index, indexed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "a4cb9244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²å¯«å…¥ 5 ç­†ç´€éŒ„ï¼Œç´¯è¨ˆï¼š35 ç­† âœ rag_results_log.xlsx\n"
     ]
    }
   ],
   "source": [
    "save_rag_log_to_excel(\n",
    "    results,\n",
    "    model_name=model_name,\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    chunking_strategy=\"hybrid_chunking\",\n",
    "    save_path=\"rag_results_log.xlsx\"\n",
    ")\n",
    "save_rag_log(\n",
    "    results,\n",
    "    model_name=model_name,\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    save_path=output_log_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c48163c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ› ï¸ å»ºç«‹æ–° index ä¸¦å„²å­˜è‡³ï¼šsentence_chunking_recursive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 316/316 [00:34<00:00,  9.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²å„²å­˜ FAISS index å’Œæ–‡ä»¶åˆ°ï¼šsentence_chunking_recursive.index / _docs.pkl\n",
      "Hybrid Chunking å®Œæˆï¼Œå…±ç”¢ç”Ÿ 3146 å€‹ chunks\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ ä¸€æ¬¡æ€§åˆå§‹åŒ–ç³»çµ±\n",
    "texts, metadatas = load_and_prepare_data(file_path)\n",
    "docs_sentence = build_documents_sentence(texts, metadatas)\n",
    "index, embedding_model, indexed_docs = load_or_build_index(\n",
    "    docs_sentence, model_name=\"all-MiniLM-L6-v2\", save_path_prefix=\"sentence_chunking_recursive\"\n",
    ")\n",
    "print(f\"sentence Chunking å®Œæˆï¼Œå…±ç”¢ç”Ÿ {len(docs_hybrid)} å€‹ chunks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a424482",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"ç”Ÿç‰©ç›¸é—œçš„è·ç¼ºæœ‰å“ªäº›\"\n",
    "results = rag_query(query, top_k, embedding_model, index, indexed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "77d2052f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²å¯«å…¥ 5 ç­†ç´€éŒ„ï¼Œç´¯è¨ˆï¼š10 ç­† âœ rag_sentence_chunking_results_log.xlsx\n"
     ]
    }
   ],
   "source": [
    "save_rag_log_to_excel(\n",
    "    results,\n",
    "    model_name=model_name,\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    chunking_strategy=\"sentence_chunking\",\n",
    "    save_path=\"rag_sentence_chunking_results_log.xlsx\"\n",
    ")\n",
    "save_rag_log(\n",
    "    results,\n",
    "    model_name=model_name,\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    save_path=\"rag_sentence_chunking_results_log.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "97cacc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ› ï¸ å»ºç«‹æ–° index ä¸¦å„²å­˜è‡³ï¼šparagraph_chunking_recursive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:10<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²å„²å­˜ FAISS index å’Œæ–‡ä»¶åˆ°ï¼šparagraph_chunking_recursive.index / _docs.pkl\n",
      "paragraph Chunking å®Œæˆï¼Œå…±ç”¢ç”Ÿ 3146 å€‹ chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "texts, metadatas = load_and_prepare_data(file_path)\n",
    "docs_sentence = build_documents_paragraph(texts, metadatas)\n",
    "index, embedding_model, indexed_docs = load_or_build_index(\n",
    "    docs_sentence, model_name=\"all-MiniLM-L6-v2\", save_path_prefix=\"paragraph_chunking_recursive\"\n",
    ")\n",
    "print(f\"paragraph Chunking å®Œæˆï¼Œå…±ç”¢ç”Ÿ {len(docs_hybrid)} å€‹ chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cc717ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²å¯«å…¥ 5 ç­†ç´€éŒ„ï¼Œç´¯è¨ˆï¼š10 ç­† âœ rag_paragraph_chunking_results_log.xlsx\n"
     ]
    }
   ],
   "source": [
    "query = \"ææ–™ç›¸é—œçš„è·ç¼ºæœ‰å“ªäº›\"\n",
    "results = rag_query(query, top_k, embedding_model, index, indexed_docs)\n",
    "save_rag_log_to_excel(\n",
    "    results,\n",
    "    model_name=model_name,\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    chunking_strategy=\"paragraph_chunking\",\n",
    "    save_path=\"rag_paragraph_chunking_results_log.xlsx\"\n",
    ")       \n",
    "save_rag_log(\n",
    "    results,\n",
    "    model_name=model_name,\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    save_path=\"rag_paragraph_chunking_results_log.csv\"\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a2efce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
