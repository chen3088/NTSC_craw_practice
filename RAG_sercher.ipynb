{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b575b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 參數設定 ===\n",
    "file_path = \"nstc_jobs_full.csv\"\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "chunk_size = 300\n",
    "chunk_overlap = 30\n",
    "encoding_batch_size = 32\n",
    "top_k = 5\n",
    "output_log_path = \"rag_results_log.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ce7de123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_and_prepare_data(file_path):\n",
    "    \"\"\"讀取爬蟲 CSV 並產生文本與中繼資料。\n",
    "    參數: file_path (str) CSV 路徑\n",
    "    回傳: texts, metadatas\n",
    "    使用: texts, metas = load_and_prepare_data('nstc_jobs_full.csv')\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.dropna(subset=[\"職缺名稱\", \"發佈日期\", \"連結\", \"詳細內容\"])\n",
    "\n",
    "    texts = [\n",
    "        f\"職缺名稱：{row['職缺名稱']}\\n發佈日期：{row['發佈日期']}\\n詳細內容：\\n{row['詳細內容']}\"\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    "\n",
    "    metadatas = [\n",
    "        {\"職缺名稱\": row[\"職缺名稱\"], \"連結\": row[\"連結\"]}\n",
    "        for _, row in df.iterrows()\n",
    "    ]\n",
    "\n",
    "    return texts, metadatas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c194caeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def build_documents(texts, metadatas, chunk_size, chunk_overlap):\n",
    "    \"\"\"將文本依指定長度切割成文件片段。\n",
    "    texts: 原始文本列表\n",
    "    metadatas: 對應的中繼資料列表\n",
    "    chunk_size: 每段最大字元數\n",
    "    chunk_overlap: 重疊字元數\n",
    "    使用: docs = build_documents(texts, metadatas, 300, 30)\n",
    "    \"\"\"\n",
    "    docs = [Document(page_content=text, metadata=meta) for text, meta in zip(texts, metadatas)]\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f72ca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def hybrid_chunking(text, metadata, chunk_size=300, chunk_overlap=30):\n",
    "    \"\"\"依標題或空行分段後再切割過長段落。\n",
    "    text: 單篇文本\n",
    "    metadata: 此文本的中繼資料\n",
    "    chunk_size: 每段最大字元\n",
    "    chunk_overlap: 段落重疊量\n",
    "    使用: chunks = hybrid_chunking(texts[0], metadatas[0])\n",
    "    \"\"\"\n",
    "    # Step 1: 先依據 【標題】或雙換行或條列符號切段\n",
    "    segments = re.split(r\"(?=【[^】]+】)|(?<=\\n)\\d+\\.\\s+|\\n{2,}\", text)\n",
    "\n",
    "    # Step 2: 對每段做長度判斷，如太長則進一步切割\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    all_chunks = []\n",
    "\n",
    "    for seg in segments:\n",
    "        if len(seg.strip()) == 0:\n",
    "            continue\n",
    "        doc = Document(page_content=seg.strip(), metadata=metadata)\n",
    "        sub_chunks = splitter.split_documents([doc])\n",
    "        all_chunks.extend(sub_chunks)\n",
    "\n",
    "    return all_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "sentence_para_chunking",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "def sentence_chunking(text, metadata):\n",
    "    \"Split text into sentences when punctuation is clear.\"\n",
    "    import re\n",
    "    sentences = re.split(r'[。.!?]\\s*', text)\n",
    "    return [Document(page_content=s.strip(), metadata=metadata) for s in sentences if s.strip()]\n",
    "\n",
    "def paragraph_chunking(text, metadata):\n",
    "    \"Split text into paragraphs separated by blank lines.\"\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "    return [Document(page_content=p, metadata=metadata) for p in paragraphs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8eb17894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "def build_faiss_index(split_docs, model_name, batch_size=32):\n",
    "    \"\"\"取得向量並建立 FAISS 索引。\n",
    "    split_docs: Document 片段列表\n",
    "    model_name: 嵌入模型名稱\n",
    "    batch_size: 批次處理量\n",
    "    使用: index, model, docs = build_faiss_index(split_docs, 'all-MiniLM-L6-v2')\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    contents = [doc.page_content for doc in split_docs]\n",
    "    embeddings = model.encode(contents, show_progress_bar=True, batch_size=batch_size)\n",
    "\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(np.array(embeddings))\n",
    "\n",
    "    return index, model, split_docs\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def load_or_build_index(docs, model_name, save_path_prefix, batch_size=32):\n",
    "    index_file = f\"{save_path_prefix}.index\"\n",
    "    docs_file = f\"{save_path_prefix}_docs.pkl\"\n",
    "\n",
    "    if os.path.exists(index_file) and os.path.exists(docs_file):\n",
    "        print(f\"📦 載入已存在的 index：{save_path_prefix}\")\n",
    "        embedding_model = SentenceTransformer(model_name)\n",
    "        index, loaded_docs = load_faiss_index(save_path_prefix)\n",
    "        return index, embedding_model, loaded_docs\n",
    "    else:\n",
    "        print(f\"🛠️ 建立新 index 並儲存至：{save_path_prefix}\")\n",
    "        index, embedding_model, split_docs = build_faiss_index(docs, model_name, batch_size)\n",
    "        save_faiss_index(index, split_docs, save_path_prefix)\n",
    "        return index, embedding_model, split_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f761d6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_faiss_index(index, docs, save_path_prefix=\"faiss_index\"):\n",
    "    # 儲存 FAISS index 本體 (.index)\n",
    "    faiss.write_index(index, f\"{save_path_prefix}.index\")\n",
    "    \n",
    "    # 儲存對應的原始 documents (.pkl)\n",
    "    with open(f\"{save_path_prefix}_docs.pkl\", \"wb\") as f:\n",
    "        pickle.dump(docs, f)\n",
    "\n",
    "    print(f\"✅ 已儲存 FAISS index 和文件到：{save_path_prefix}.index / _docs.pkl\")\n",
    "def load_faiss_index(save_path_prefix=\"faiss_index\"):\n",
    "    # 讀取 FAISS index\n",
    "    index = faiss.read_index(f\"{save_path_prefix}.index\")\n",
    "    \n",
    "    # 讀取原始文件 chunks\n",
    "    with open(f\"{save_path_prefix}_docs.pkl\", \"rb\") as f:\n",
    "        docs = pickle.load(f)\n",
    "\n",
    "    print(f\"📂 成功載入 index 與 documents，筆數：{len(docs)}\")\n",
    "    return index, docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46c3f6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(query, top_k, embedding_model, index, indexed_docs):\n",
    "    \"\"\"在向量索引中取得相關片段。\n",
    "    query: 查詢字串\n",
    "    top_k: 回傳前幾筆結果\n",
    "    embedding_model: 用來編碼查詢的模型\n",
    "    index: FAISS 索引\n",
    "    indexed_docs: 與索引對應的文件片段\n",
    "    使用: results = rag_query('關鍵字', 5, model, index, docs)\n",
    "    \"\"\"\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    D, I = index.search(np.array(query_embedding), top_k)\n",
    "\n",
    "    results = []\n",
    "    for i, idx in enumerate(I[0]):\n",
    "        doc = indexed_docs[idx]\n",
    "        snippet = doc.page_content.strip().replace(\"\\n\", \" \")\n",
    "        if len(snippet) > 200:\n",
    "            snippet = snippet[:200] + \"...\"\n",
    "\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"rank\": i + 1,\n",
    "            \"職缺名稱\": doc.metadata[\"職缺名稱\"],\n",
    "            \"連結\": doc.metadata[\"連結\"],\n",
    "            \"摘要\": snippet\n",
    "        })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2d388e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def save_rag_log(results, model_name, chunk_size, chunk_overlap, save_path=\"rag_results_log.csv\"):\n",
    "    \"\"\"將查詢結果寫入 CSV 檔。\n",
    "    results: rag_query 產生的結果\n",
    "    model_name: 使用的模型名稱\n",
    "    chunk_size: 切割長度設定\n",
    "    chunk_overlap: 重疊長度設定\n",
    "    save_path: 儲存的 CSV 路徑\n",
    "    使用: save_rag_log(results, model_name, chunk_size, chunk_overlap)\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    write_header = not os.path.exists(save_path)\n",
    "\n",
    "    with open(save_path, \"a\", newline='', encoding=\"utf-8-sig\") as f:  # 👈 修正編碼為 utf-8-sig\n",
    "        writer = csv.DictWriter(\n",
    "            f,\n",
    "            fieldnames=[\"timestamp\", \"query\", \"rank\", \"職缺名稱\", \"連結\", \"摘要\", \"model_name\", \"chunk_size\", \"chunk_overlap\"]\n",
    "        )\n",
    "        if write_header:\n",
    "            writer.writeheader()\n",
    "        for row in results:\n",
    "            writer.writerow({\n",
    "                \"timestamp\": timestamp,\n",
    "                \"query\": row[\"query\"],\n",
    "                \"rank\": row[\"rank\"],\n",
    "                \"職缺名稱\": row[\"職缺名稱\"],\n",
    "                \"連結\": row[\"連結\"],\n",
    "                \"摘要\": row[\"摘要\"],\n",
    "                \"model_name\": model_name,\n",
    "                \"chunk_size\": chunk_size,\n",
    "                \"chunk_overlap\": chunk_overlap\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56ebd91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def save_rag_log_to_excel(results, model_name, chunk_size, chunk_overlap, chunking_strategy, save_path=\"rag_results_log.xlsx\"):\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # 將本次查詢結果轉成 DataFrame\n",
    "    new_df = pd.DataFrame([{\n",
    "        \"timestamp\": timestamp,\n",
    "        \"query\": row[\"query\"],\n",
    "        \"rank\": row[\"rank\"],\n",
    "        \"職缺名稱\": row[\"職缺名稱\"],\n",
    "        \"連結\": row[\"連結\"],\n",
    "        \"摘要\": row[\"摘要\"],\n",
    "        \"model_name\": model_name,\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"chunk_overlap\": chunk_overlap,\n",
    "        \"chunking_strategy\": chunking_strategy\n",
    "    } for row in results])\n",
    "\n",
    "    # 如果檔案已存在就讀入合併\n",
    "    if os.path.exists(save_path):\n",
    "        old_df = pd.read_excel(save_path)\n",
    "        combined_df = pd.concat([old_df, new_df], ignore_index=True)\n",
    "    else:\n",
    "        combined_df = new_df\n",
    "\n",
    "    # 寫入 Excel\n",
    "    combined_df.to_excel(save_path, index=False)\n",
    "    print(f\"✅ 已寫入 {len(new_df)} 筆紀錄，累計：{len(combined_df)} 筆 ➜ {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "345700e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 載入已存在的 index：faiss_recursive\n",
      "📂 成功載入 index 與 documents，筆數：3146\n"
     ]
    }
   ],
   "source": [
    "# 🚀 一次性初始化系統\n",
    "texts, metadatas = load_and_prepare_data(file_path)\n",
    "docs = hybrid_chunking(texts[0], metadatas[0], chunk_size, chunk_overlap)\n",
    "index, embedding_model, indexed_docs = load_or_build_index(\n",
    "    docs, model_name=\"all-MiniLM-L6-v2\", save_path_prefix=\"faiss_recursive\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "65c18bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(query, top_k, embedding_model, index, indexed_docs):\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    D, I = index.search(np.array(query_embedding), top_k)\n",
    "\n",
    "    results = []\n",
    "    for i, idx in enumerate(I[0]):\n",
    "        if idx >= len(indexed_docs):\n",
    "            print(f\"⚠️ 無效索引：{idx} 超出 indexed_docs 範圍（{len(indexed_docs)}）\")\n",
    "            continue  # 跳過錯誤索引\n",
    "\n",
    "        doc = indexed_docs[idx]\n",
    "        snippet = doc.page_content.strip().replace(\"\\n\", \" \")\n",
    "        if len(snippet) > 200:\n",
    "            snippet = snippet[:200] + \"...\"\n",
    "\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"rank\": i + 1,\n",
    "            \"職缺名稱\": doc.metadata[\"職缺名稱\"],\n",
    "            \"連結\": doc.metadata[\"連結\"],\n",
    "            \"摘要\": snippet\n",
    "        })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8b31ab79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query': '生物相關的職缺有哪些',\n",
       "  'rank': 1,\n",
       "  '職缺名稱': '中國醫藥大學生物醫學研究所誠徵博士後研究員',\n",
       "  '連結': 'https://www.nstc.gov.tw/folksonomy/detail/cc8706e2-836d-4f85-94d2-89396360a823?l=ch',\n",
       "  '摘要': '生物醫學相關'},\n",
       " {'query': '生物相關的職缺有哪些',\n",
       "  'rank': 2,\n",
       "  '職缺名稱': '馬偕紀念醫院血液腫瘤科蘇迺文醫師誠徵國科會補助計畫專任助理',\n",
       "  '連結': 'https://www.nstc.gov.tw/folksonomy/detail/a78b7d93-b5b4-4bb9-bff5-5888b2d695e6?l=ch',\n",
       "  '摘要': '生命科學相關系所'},\n",
       " {'query': '生物相關的職缺有哪些',\n",
       "  'rank': 3,\n",
       "  '職缺名稱': '高雄榮總教研部生殖暨粒線體醫學研究室---誠徵博士後研究員',\n",
       "  '連結': 'https://www.nstc.gov.tw/folksonomy/detail/47729f59-955a-4b43-addd-5a18d1affa86?l=ch',\n",
       "  '摘要': '生物醫學相關領域畢業。'},\n",
       " {'query': '生物相關的職缺有哪些',\n",
       "  'rank': 4,\n",
       "  '職缺名稱': '台大醫院耳鼻喉部楊宗霖教授徵博士後研究員',\n",
       "  '連結': 'https://www.nstc.gov.tw/folksonomy/detail/e2f4f22d-3604-4ce0-854e-94b9a0ce8c10?l=ch',\n",
       "  '摘要': '細胞生物相關技術'},\n",
       " {'query': '生物相關的職缺有哪些',\n",
       "  'rank': 5,\n",
       "  '職缺名稱': '國家衛生研究院癌症研究所 誠徵院內博士後研究員或研究助理一名',\n",
       "  '連結': 'https://www.nstc.gov.tw/folksonomy/detail/95244417-fdb2-451f-812c-315ae9e234c4?l=ch',\n",
       "  '摘要': '具有生化、細胞、分生背景及細胞培養等相關研究經驗。'}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"生物相關的職缺有哪些\"\n",
    "rag_query(query, top_k, embedding_model, index, indexed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a4cb9244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已寫入 5 筆紀錄，累計：15 筆 ➜ rag_results_log.xlsx\n"
     ]
    }
   ],
   "source": [
    "save_rag_log_to_excel(\n",
    "    results,\n",
    "    model_name=model_name,\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    chunking_strategy=\"hybrid_chunking\",\n",
    "    save_path=\"rag_results_log.xlsx\"\n",
    ")\n",
    "save_rag_log(\n",
    "    results,\n",
    "    model_name=model_name,\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    save_path=output_log_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c48163c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
